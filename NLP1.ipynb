{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1V_XouX2_ctWclvm5ZYYG8mPcqAnE_-sT",
      "authorship_tag": "ABX9TyM4HkWoOgwyYFxK1uuL1g/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yardenmizrahi/NLP1/blob/main/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP homework 1:**\n",
        "\n",
        "This assignment will provide hands-on practice with text processing techniques in Python,\n",
        "including tokenization, lemmatization, and stemming. You will also gain experience loading,\n",
        "analyzing, and scraping textual data from different sources.\n"
      ],
      "metadata": {
        "id": "3MueA80OLv2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Yarden Mizrahi - 209521293*"
      ],
      "metadata": {
        "id": "GKW3tUVQMHBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Python Libraries**"
      ],
      "metadata": {
        "id": "Pl8LiGRMMrBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNfe79k9Lf1b"
      },
      "outputs": [],
      "source": [
        "# import necessary python libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading The Data**"
      ],
      "metadata": {
        "id": "myLNtOqmMfTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Data/spam.csv\""
      ],
      "metadata": {
        "id": "DAToGYErMPOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a0944a-22f6-4cd8-8257-8ec47368f5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading & Basic Analysis**"
      ],
      "metadata": {
        "id": "CExw7dmdQQMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Download nltk resources if not already present\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(path, encoding='latin-1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vjod5FiN-zZ",
        "outputId": "ecb060b6-6bb6-49c8-875c-8b12f8f95408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "basic statistics on the data:"
      ],
      "metadata": {
        "id": "QgYsu87UTZT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics\n",
        "total_messages = len(df)\n",
        "spam_messages = len(df[df['v1'] == 'spam'])\n",
        "ham_messages = len(df[df['v1'] == 'ham'])\n",
        "\n",
        "print(\"Total number of SMS messages:\", total_messages)\n",
        "print(\"Number of spam messages:\", spam_messages)\n",
        "print(\"Number of ham messages:\", ham_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqDlf19gQMZR",
        "outputId": "8230825c-b959-4a77-8a8d-102ba0ba8e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of SMS messages: 5572\n",
            "Number of spam messages: 747\n",
            "Number of ham messages: 4825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average number of words per message\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df['word_count'] = df['v2'].apply(lambda x: len(word_tokenize(x)))\n",
        "average_words_per_message = df['word_count'].mean()\n",
        "print(\"Average number of words per message:\", average_words_per_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3jNFW5cQWPD",
        "outputId": "4c5ae264-7fb1-4151-f862-16903db979b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of words per message: 18.699389806173727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize, remove stopwords, and lemmatize words\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "all_words = []\n",
        "for message in df['v2']:\n",
        "    words = word_tokenize(message.lower())\n",
        "    filtered_words = [wordnet_lemmatizer.lemmatize(w) for w in words if w.isalnum() and w not in stop_words and w not in punctuation and w.isalpha()]\n",
        "    all_words.extend(filtered_words)\n",
        "\n",
        "# Calculate frequency distribution of words\n",
        "fdist = FreqDist(all_words)\n",
        "\n",
        "# Print 5 most frequent words\n",
        "print(\"5 most frequent words:\")\n",
        "for word, frequency in fdist.most_common(5):\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov25Ua6nQ3wQ",
        "outputId": "6d39750a-a160-4bb6-9554-f5be34c6441c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 most frequent words:\n",
            "u: 1184\n",
            "call: 603\n",
            "get: 396\n",
            "ur: 381\n",
            "gt: 318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of words that appear only once\n",
        "unique_words = [word for word, frequency in fdist.items() if frequency == 1]\n",
        "print(\"Number of words that only appear once:\", len(unique_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Hi9Xl4Rla8",
        "outputId": "0c565fd4-bd77-4725-ed21-49a6490dec88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words that only appear once: 3293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Processing**"
      ],
      "metadata": {
        "id": "IUdtKlOwSs0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the SMS text using both nltk and spaCy. Analyze the time complexity of the tokenization algorithm\n"
      ],
      "metadata": {
        "id": "7v1K6arQTNZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using NLTK\n",
        "def tokenize_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum() and word not in stop_words and word not in punctuation]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Example usage\n",
        "tokens_nltk = tokenize_nltk(df['v2'][0])\n",
        "print(\"NLTK Tokens:\", tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wez-4LwjSOMe",
        "outputId": "05bf3e4e-0557-4d63-cbff-50957406204f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens: ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Tokenize using spaCy\n",
        "def tokenize_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Example usage\n",
        "tokens_spacy = tokenize_spacy(df['v2'][0])\n",
        "print(\"spaCy Tokens:\", tokens_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P_W-N0-Ut5Q",
        "outputId": "a9bcab3a-c9be-478f-c4c5-f2b7cd581a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Tokens: ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time complexity of the tokenization algorithms:\n",
        "\n",
        "1. NLTK Tokenization:\n",
        "   NLTK's `word_tokenize` function is based on regular expressions and it tokenizes text by splitting it into words based on whitespace and punctuation. The time complexity of NLTK's tokenization algorithm is generally considered to be linear, O(n), where n is the length of the input text.\n",
        "\n",
        "2. spaCy Tokenization:\n",
        "   spaCy's tokenization is based on a statistical model trained on large corpora of text. It uses a combination of rules and heuristics to segment text into tokens. The time complexity of spaCy's tokenization algorithm is typically linear or close to linear, O(n), where n is the length of the input text."
      ],
      "metadata": {
        "id": "1qMoJsNaUA4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatize the SMS text using nltk and spaCy. Analyze the time complexity of the lemmatization algorithm\n"
      ],
      "metadata": {
        "id": "IOpRICkAU6bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize using NLTK\n",
        "def lemmatize_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Example usage\n",
        "lemmatized_nltk = lemmatize_nltk(df['v2'][0])\n",
        "print(\"NLTK Lemmatized Tokens:\", lemmatized_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjGRbfuKU5bG",
        "outputId": "7d7a7a4e-0e8f-4b0d-c67d-89e76127c5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Lemmatized Tokens: ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize using spaCy\n",
        "def lemmatize_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Example usage\n",
        "lemmatized_spacy = lemmatize_spacy(df['v2'][0])\n",
        "print(\"spaCy Lemmatized Tokens:\", lemmatized_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHrDyheGTMcL",
        "outputId": "3c9b75e7-88e9-44ba-f16a-1a0bd933b29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Lemmatized Tokens: ['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time complexity of the lemmatization algorithms:\n",
        "\n",
        "1. **NLTK Lemmatization:**\n",
        "   NLTK's lemmatization algorithm is based on WordNet, a lexical database of English words. NLTK's `WordNetLemmatizer` applies morphological analysis to reduce words to their base or dictionary form (lemma). The time complexity of NLTK's lemmatization algorithm depends on the length of the input text and the number of tokens. Since NLTK's lemmatization operates on individual tokens independently, its time complexity can be considered linear, O(n), where n is the number of tokens in the input text.\n",
        "\n",
        "2. **spaCy Lemmatization:**\n",
        "   spaCy's lemmatization algorithm is integrated into its linguistic processing pipeline. It utilizes pre-trained statistical models to perform lemmatization based on contextual information and linguistic rules. spaCy's lemmatization process involves analyzing the syntactic structure of the input text and mapping words to their base forms using the information encoded in the model. The time complexity of spaCy's lemmatization algorithm is influenced by the length of the input text and the complexity of the linguistic processing involved. While the exact time complexity may vary depending on the specific implementation and the characteristics of the input text, spaCy's lemmatization is generally efficient and can often be considered linear or close to linear in practice."
      ],
      "metadata": {
        "id": "tc3-MiDGWlNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stem the SMS text using nltk and spaCy. Analyze the time complexity of the stemming algorithm\n"
      ],
      "metadata": {
        "id": "WHYzHTagXrK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem using NLTK\n",
        "def stem_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Example usage\n",
        "stemmed_nltk = stem_nltk(df['v2'][0])\n",
        "print(\"NLTK Stemmed Tokens:\", stemmed_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fMNCFL-VG-n",
        "outputId": "a44653e5-6d10-4517-f313-2ecc5b634a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Stemmed Tokens: ['go', 'until', 'jurong', 'point', ',', 'crazi', '..', 'avail', 'onli', 'in', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amor', 'wat', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the stemmer\n",
        "if not nlp.has_pipe(\"stemmer\"):\n",
        "    print(\"Error: The spaCy model does not include a stemmer.\")\n",
        "else:\n",
        "  stemmer = nlp.stem\n",
        "\n",
        "  # Stem using spaCy\n",
        "  def stem_spacy(text):\n",
        "      doc = nlp(text)\n",
        "      stemmed_tokens = [stemmer(token.text) for token in doc]\n",
        "      return stemmed_tokens\n",
        "\n",
        "  # Example usage\n",
        "  stemmed_spacy = stem_spacy(df['v2'][0])\n",
        "  print(\"spaCy Stemmed Tokens:\", stemmed_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0djQ6JWVbDVl",
        "outputId": "24ba7f01-94d7-4cf7-f584-a4c47f52cfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The spaCy model does not include a stemmer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time complexity of the stemming algorithms:\n",
        "\n",
        "1. **NLTK Stemming:**\n",
        "   NLTK's Porter Stemmer algorithm is based on Porter's stemming algorithm, which applies a series of heuristic rules to remove suffixes from words to obtain their root forms (stems). The time complexity of NLTK's stemming algorithm depends on the length of the input text and the number of tokens. Since NLTK's stemming operates on individual tokens independently, its time complexity can be considered linear, O(n), where n is the number of tokens in the input text. However, the actual performance may vary depending on the complexity of the stemming rules and the length of the input text.\n",
        "\n",
        "2. **spaCy Stemming:**  The spaCy model does not include a stemmer."
      ],
      "metadata": {
        "id": "toF6zc3nfD0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For each technique, write 2-3 sentences comparing the nltk and spaCy implementation. Consider things like output format, processing speed, language support etc."
      ],
      "metadata": {
        "id": "FTg73zTciBnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "- **NLTK:**\n",
        "  - **Output Format:** The `word_tokenize` function in NLTK returns a list of strings, where each string is a token.\n",
        "  - **Processing Speed:** NLTK's tokenization is relatively fast for small to medium-sized texts, but it might be slower for very large texts due to its rule-based approach.\n",
        "  - **Language Support:** Primarily designed for English, although there are some tokenizers for other languages.\n",
        "\n",
        "- **spaCy:**\n",
        "  - **Output Format:** spaCy returns a `Doc` object that contains tokens, which can be accessed as attributes. Each token has additional linguistic information attached.\n",
        "  - **Processing Speed:** spaCy is optimized for performance and can handle large texts efficiently due to its use of compiled code and optimized algorithms.\n",
        "  - **Language Support:** spaCy provides robust support for multiple languages with pre-trained models for various languages.\n",
        "\n",
        "### Lemmatization\n",
        "\n",
        "- **NLTK:**\n",
        "  - **Output Format:** The `WordNetLemmatizer` in NLTK returns a list of lemmatized words as strings.\n",
        "  - **Processing Speed:** NLTK's lemmatization can be slower than spaCy because it relies on querying the WordNet database, which can introduce overhead.\n",
        "  - **Language Support:** Mainly supports English, leveraging the WordNet database.\n",
        "\n",
        "- **spaCy:**\n",
        "  - **Output Format:** Lemmatization in spaCy is part of the token attributes, accessed via `token.lemma_`. The output is a list of lemmas.\n",
        "  - **Processing Speed:** spaCy's lemmatization is generally faster due to its integrated pipeline and optimized processing.\n",
        "  - **Language Support:** spaCy provides extensive language support with lemmatization models for multiple languages.\n",
        "\n",
        "### Stemming\n",
        "\n",
        "- **NLTK:**\n",
        "  - **Output Format:** The `PorterStemmer` in NLTK returns a list of stemmed tokens as strings.\n",
        "  - **Processing Speed:** Stemming in NLTK is relatively fast due to its rule-based approach, but it can be less accurate.\n",
        "  - **Language Support:** Primarily supports English with various stemmers available (Porter, Lancaster, Snowball).\n",
        "\n",
        "- **spaCy:**\n",
        "  - **Output Format:** spaCy does not provide a direct stemming functionality. Instead, it focuses on lemmatization, which can serve a similar purpose.\n",
        "  - **Processing Speed:** N/A for stemming.\n",
        "  - **Language Support:** N/A for stemming.\n"
      ],
      "metadata": {
        "id": "8qIz0UcohPuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print updated statistics on word count and frequent words after applying each technique."
      ],
      "metadata": {
        "id": "9sED75vbiHxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['v1', 'v2']]  # Keep only the relevant columns\n",
        "\n",
        "# Initialize stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)"
      ],
      "metadata": {
        "id": "idqztOVcb-WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing Functions**\n"
      ],
      "metadata": {
        "id": "HV7AZYDyjLk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Tokenization, Lemmatization, and Stemming**"
      ],
      "metadata": {
        "id": "I6bugCsMjPjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLTK tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenization, Lemmatization, and Stemming with NLTK\n",
        "def preprocess_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum() and word not in stop_words and word not in punctuation]\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return tokens, lemmas, stems\n"
      ],
      "metadata": {
        "id": "4prOufCkjShg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy Tokenization and Lemmatization**"
      ],
      "metadata": {
        "id": "3WJuV31tjWor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Lemmatization with spaCy\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "    return tokens, lemmas"
      ],
      "metadata": {
        "id": "Fgl9zV0ujWJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Update Statistics Functions**"
      ],
      "metadata": {
        "id": "aUyslygAjbHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def update_statistics(tokens_list):\n",
        "    word_counts = Counter(tokens_list)\n",
        "    total_words = sum(word_counts.values())\n",
        "    unique_words = len(word_counts)\n",
        "    most_common_words = word_counts.most_common(5)\n",
        "    words_appearing_once = sum(1 for count in word_counts.values() if count == 1)\n",
        "    return total_words, unique_words, most_common_words, words_appearing_once\n",
        "\n",
        "def print_statistics(method_name, tokens, lemmas, stems=None):\n",
        "    print(f\"Statistics for {method_name}:\")\n",
        "\n",
        "    total_words, unique_words, most_common_words, words_appearing_once = update_statistics(tokens)\n",
        "    print(f\"  Total words: {total_words}\")\n",
        "    print(f\"  Unique words: {unique_words}\")\n",
        "    print(f\"  Most common words: {most_common_words}\")\n",
        "    print(f\"  Words appearing once: {words_appearing_once}\")\n",
        "\n",
        "    total_lemmas, unique_lemmas, most_common_lemmas, lemmas_appearing_once = update_statistics(lemmas)\n",
        "    print(f\"  Total lemmas: {total_lemmas}\")\n",
        "    print(f\"  Unique lemmas: {unique_lemmas}\")\n",
        "    print(f\"  Most common lemmas: {most_common_lemmas}\")\n",
        "    print(f\"  Lemmas appearing once: {lemmas_appearing_once}\")\n",
        "\n",
        "    if stems:\n",
        "        total_stems, unique_stems, most_common_stems, stems_appearing_once = update_statistics(stems)\n",
        "        print(f\"  Total stems: {total_stems}\")\n",
        "        print(f\"  Unique stems: {unique_stems}\")\n",
        "        print(f\"  Most common stems: {most_common_stems}\")\n",
        "        print(f\"  Stems appearing once: {stems_appearing_once}\")\n"
      ],
      "metadata": {
        "id": "bsrEP4rBjcmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Applying Preprocessing and Printing Statistics**"
      ],
      "metadata": {
        "id": "M-Z-rqUajft6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens, nltk_lemmas, nltk_stems = [], [], []\n",
        "spacy_tokens, spacy_lemmas = [], []\n",
        "\n",
        "for message in df['v2']:\n",
        "    tokens, lemmas, stems = preprocess_nltk(message)\n",
        "    nltk_tokens.extend(tokens)\n",
        "    nltk_lemmas.extend(lemmas)\n",
        "    nltk_stems.extend(stems)\n",
        "\n",
        "    tokens, lemmas = preprocess_spacy(message)\n",
        "    spacy_tokens.extend(tokens)\n",
        "    spacy_lemmas.extend(lemmas)\n",
        "\n",
        "print_statistics(\"NLTK\", nltk_tokens, nltk_lemmas, nltk_stems)\n",
        "print_statistics(\"spaCy\", spacy_tokens, spacy_lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZnJ7DEbji0Q",
        "outputId": "da1a230f-0a9e-464e-c9a7-281a739f2126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics for NLTK:\n",
            "  Total words: 56617\n",
            "  Unique words: 8188\n",
            "  Most common words: [('i', 1956), ('u', 1133), ('call', 576), ('2', 485), ('get', 385)]\n",
            "  Words appearing once: 4107\n",
            "  Total lemmas: 56617\n",
            "  Unique lemmas: 7661\n",
            "  Most common lemmas: [('i', 1956), ('u', 1197), ('call', 603), ('2', 485), ('get', 396)]\n",
            "  Lemmas appearing once: 3803\n",
            "  Total stems: 56617\n",
            "  Unique stems: 6861\n",
            "  Most common stems: [('i', 1956), ('u', 1133), ('call', 656), ('2', 485), ('go', 451)]\n",
            "  Stems appearing once: 3298\n",
            "Statistics for spaCy:\n",
            "  Total words: 41172\n",
            "  Unique words: 7036\n",
            "  Most common words: [('u', 1098), ('ur', 380), ('nt', 319), ('free', 283), ('ok', 282)]\n",
            "  Words appearing once: 3608\n",
            "  Total lemmas: 41172\n",
            "  Unique lemmas: 6139\n",
            "  Most common lemmas: [('u', 1098), ('ur', 380), ('go', 329), ('come', 325), ('not', 313)]\n",
            "  Lemmas appearing once: 3128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Web Scraping**"
      ],
      "metadata": {
        "id": "0CAfhnJ43k1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use BeautifulSoup to scrape text data from a public page on one of your social media profiles."
      ],
      "metadata": {
        "id": "VTpZyXCWko9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the page to scrape - my github repository \"boris\" readme file\n",
        "url = 'https://github.com/yardenmizrahi/Boris'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "   # Find the main content div\n",
        "    content_div = soup.find(\"div\", {\"class\": \"Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned\"})\n",
        "\n",
        "    # Extract the text from the content div\n",
        "    text_content = content_div.text\n",
        "\n",
        "    print(text_content)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Gk-EKijvEF",
        "outputId": "ac64f6ca-81ea-4f05-8f06-e4e2d4982596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel Implementation of Proximity Criteria\n",
            "Final project\n",
            "Course 10324, Parallel and Distributed Computation\n",
            "2023 Fall Semester\n",
            "A set of N points is placed in two-dimensional plane. Coordinates (x, y) of each point P are defined as follows:\n",
            "x = ((x2 â€“ x1) / 2 ) * sin (tÏ€ /2) + (x2 + x1) / 2)\n",
            "y = ax + b\n",
            "where (x1, x2, a, b) are constant parameters predefined for each point P.\n",
            "Problem Definition\n",
            "We will say that some point P from the set satisfies a Proximity Criteria if there exist at least K points in the set with a distance from the point P less than a given value D.\n",
            "Given a value of parameter t, we want to find if there exist at least 3 points that satisfies the Proximity Criteria\n",
            "Requirements\n",
            "â€¢\tPerform checks for Proximity Criteria for tCount + 1 values of  t:\n",
            "t = 2 * i / tCount  - 1,          i = 0,  1,  2,  3, â€¦,  tCount\n",
            "where tCount is a given integer number.\n",
            "â€¢\tFor each value of t find if there is three points that satisfy the Proximity Criteria. If such three points are found â€“ don't continue evaluation for this specific value of t.\n",
            "â€¢\tThe input file input.txt initially is known for one process only. The results must be written to the file output.txt by the same process.\n",
            "â€¢\tThe computation time of the parallel program must be faster than sequential solution.\n",
            "â€¢\tBe ready to demonstrate your solution running on VLAB (two computers from different pools when using MPI)\n",
            "â€¢\tNo code sharing between students is allowed. Each part of code, if any, which was incorporated into your project must be referenced according to the academic rules.\n",
            "â€¢\tBe able to explain each line of the project code, including those that were reused from any source.\n",
            "â€¢\tThe project that is not created properly (missing files, build or run errors) will not be accepted\n",
            "Input data and Output Result of the project\n",
            "The input file contains N in the first line - the number of point in the set, K â€“ minimal number of points to satisfy the Proximity Criteria, distance D  and TCount. T\n",
            "he next N lines contain parameters for every point in the set. One or more blanks are between the numbers in a file.\n",
            "Output.txt\n",
            "The output file contains information about results found for points that satisfies the Proximity Criteria.\n",
            "â€¢\tFor each t that 3 points satisfying the Proximity Criteria were found, it contains a line with the parameter t and ID of these 3 points\n",
            "Points  pointID1, pointID2, pointID3 satisfy Proximity Criteria at t = t1\n",
            "Points  pointID4, pointID5, pointID6 satisfy Proximity Criteria at t = t2\n",
            "Points  pointID7, pointID8, pointID9 satisfy Proximity Criteria at t = t3\n",
            "â€¢\tIn case that the points were not found for any t, the program outputs:\n",
            "Number of points\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform tokenization, lemmatization, and stemming on the scraped text"
      ],
      "metadata": {
        "id": "dzI-FF3irVwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "copy_text_content = text_content\n",
        "# Tokenization\n",
        "tokens = word_tokenize(copy_text_content)\n",
        "tokens = [word.lower() for word in tokens if word.isalnum() and word not in stop_words and word not in punctuation]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Lemmatization\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmas:\", lemmas)\n",
        "\n",
        "# Stemming\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stems:\", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHlmVld7k5_L",
        "outputId": "41a6f817-942d-4e45-944b-a21ed35afcf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['parallel', 'implementation', 'proximity', 'criteria', 'final', 'project', 'course', '10324', 'parallel', 'distributed', 'computation', '2023', 'fall', 'semester', 'a', 'set', 'n', 'points', 'placed', 'plane', 'coordinates', 'x', 'point', 'p', 'defined', 'follows', 'x', 'x2', 'x1', '2', 'sin', 'tÏ€', 'x2', 'x1', '2', 'ax', 'b', 'x1', 'x2', 'b', 'constant', 'parameters', 'predefined', 'point', 'problem', 'definition', 'we', 'say', 'point', 'p', 'set', 'satisfies', 'proximity', 'criteria', 'exist', 'least', 'k', 'points', 'set', 'distance', 'point', 'p', 'less', 'given', 'value', 'given', 'value', 'parameter', 'want', 'find', 'exist', 'least', '3', 'points', 'satisfies', 'proximity', 'criteria', 'requirements', 'perform', 'checks', 'proximity', 'criteria', 'tcount', '1', 'values', '2', 'tcount', '1', '0', '1', '2', '3', 'tcount', 'tcount', 'given', 'integer', 'number', 'for', 'value', 'find', 'three', 'points', 'satisfy', 'proximity', 'criteria', 'if', 'three', 'points', 'found', 'continue', 'evaluation', 'specific', 'value', 'the', 'input', 'file', 'initially', 'known', 'one', 'process', 'the', 'results', 'must', 'written', 'file', 'process', 'the', 'computation', 'time', 'parallel', 'program', 'must', 'faster', 'sequential', 'solution', 'be', 'ready', 'demonstrate', 'solution', 'running', 'vlab', 'two', 'computers', 'different', 'pools', 'using', 'mpi', 'no', 'code', 'sharing', 'students', 'allowed', 'each', 'part', 'code', 'incorporated', 'project', 'must', 'referenced', 'according', 'academic', 'rules', 'be', 'able', 'explain', 'line', 'project', 'code', 'including', 'reused', 'source', 'the', 'project', 'created', 'properly', 'missing', 'files', 'build', 'run', 'errors', 'accepted', 'input', 'data', 'output', 'result', 'project', 'the', 'input', 'file', 'contains', 'n', 'first', 'line', 'number', 'point', 'set', 'k', 'minimal', 'number', 'points', 'satisfy', 'proximity', 'criteria', 'distance', 'd', 'tcount', 't', 'next', 'n', 'lines', 'contain', 'parameters', 'every', 'point', 'set', 'one', 'blanks', 'numbers', 'file', 'the', 'output', 'file', 'contains', 'information', 'results', 'found', 'points', 'satisfies', 'proximity', 'criteria', 'for', '3', 'points', 'satisfying', 'proximity', 'criteria', 'found', 'contains', 'line', 'parameter', 'id', '3', 'points', 'points', 'pointid1', 'pointid2', 'pointid3', 'satisfy', 'proximity', 'criteria', 't1', 'points', 'pointid4', 'pointid5', 'pointid6', 'satisfy', 'proximity', 'criteria', 't2', 'points', 'pointid7', 'pointid8', 'pointid9', 'satisfy', 'proximity', 'criteria', 't3', 'in', 'case', 'points', 'found', 'program', 'outputs', 'number', 'points']\n",
            "Lemmas: ['parallel', 'implementation', 'proximity', 'criterion', 'final', 'project', 'course', '10324', 'parallel', 'distributed', 'computation', '2023', 'fall', 'semester', 'a', 'set', 'n', 'point', 'placed', 'plane', 'coordinate', 'x', 'point', 'p', 'defined', 'follows', 'x', 'x2', 'x1', '2', 'sin', 'tÏ€', 'x2', 'x1', '2', 'ax', 'b', 'x1', 'x2', 'b', 'constant', 'parameter', 'predefined', 'point', 'problem', 'definition', 'we', 'say', 'point', 'p', 'set', 'satisfies', 'proximity', 'criterion', 'exist', 'least', 'k', 'point', 'set', 'distance', 'point', 'p', 'le', 'given', 'value', 'given', 'value', 'parameter', 'want', 'find', 'exist', 'least', '3', 'point', 'satisfies', 'proximity', 'criterion', 'requirement', 'perform', 'check', 'proximity', 'criterion', 'tcount', '1', 'value', '2', 'tcount', '1', '0', '1', '2', '3', 'tcount', 'tcount', 'given', 'integer', 'number', 'for', 'value', 'find', 'three', 'point', 'satisfy', 'proximity', 'criterion', 'if', 'three', 'point', 'found', 'continue', 'evaluation', 'specific', 'value', 'the', 'input', 'file', 'initially', 'known', 'one', 'process', 'the', 'result', 'must', 'written', 'file', 'process', 'the', 'computation', 'time', 'parallel', 'program', 'must', 'faster', 'sequential', 'solution', 'be', 'ready', 'demonstrate', 'solution', 'running', 'vlab', 'two', 'computer', 'different', 'pool', 'using', 'mpi', 'no', 'code', 'sharing', 'student', 'allowed', 'each', 'part', 'code', 'incorporated', 'project', 'must', 'referenced', 'according', 'academic', 'rule', 'be', 'able', 'explain', 'line', 'project', 'code', 'including', 'reused', 'source', 'the', 'project', 'created', 'properly', 'missing', 'file', 'build', 'run', 'error', 'accepted', 'input', 'data', 'output', 'result', 'project', 'the', 'input', 'file', 'contains', 'n', 'first', 'line', 'number', 'point', 'set', 'k', 'minimal', 'number', 'point', 'satisfy', 'proximity', 'criterion', 'distance', 'd', 'tcount', 't', 'next', 'n', 'line', 'contain', 'parameter', 'every', 'point', 'set', 'one', 'blank', 'number', 'file', 'the', 'output', 'file', 'contains', 'information', 'result', 'found', 'point', 'satisfies', 'proximity', 'criterion', 'for', '3', 'point', 'satisfying', 'proximity', 'criterion', 'found', 'contains', 'line', 'parameter', 'id', '3', 'point', 'point', 'pointid1', 'pointid2', 'pointid3', 'satisfy', 'proximity', 'criterion', 't1', 'point', 'pointid4', 'pointid5', 'pointid6', 'satisfy', 'proximity', 'criterion', 't2', 'point', 'pointid7', 'pointid8', 'pointid9', 'satisfy', 'proximity', 'criterion', 't3', 'in', 'case', 'point', 'found', 'program', 'output', 'number', 'point']\n",
            "Stems: ['parallel', 'implement', 'proxim', 'criteria', 'final', 'project', 'cours', '10324', 'parallel', 'distribut', 'comput', '2023', 'fall', 'semest', 'a', 'set', 'n', 'point', 'place', 'plane', 'coordin', 'x', 'point', 'p', 'defin', 'follow', 'x', 'x2', 'x1', '2', 'sin', 'tÏ€', 'x2', 'x1', '2', 'ax', 'b', 'x1', 'x2', 'b', 'constant', 'paramet', 'predefin', 'point', 'problem', 'definit', 'we', 'say', 'point', 'p', 'set', 'satisfi', 'proxim', 'criteria', 'exist', 'least', 'k', 'point', 'set', 'distanc', 'point', 'p', 'less', 'given', 'valu', 'given', 'valu', 'paramet', 'want', 'find', 'exist', 'least', '3', 'point', 'satisfi', 'proxim', 'criteria', 'requir', 'perform', 'check', 'proxim', 'criteria', 'tcount', '1', 'valu', '2', 'tcount', '1', '0', '1', '2', '3', 'tcount', 'tcount', 'given', 'integ', 'number', 'for', 'valu', 'find', 'three', 'point', 'satisfi', 'proxim', 'criteria', 'if', 'three', 'point', 'found', 'continu', 'evalu', 'specif', 'valu', 'the', 'input', 'file', 'initi', 'known', 'one', 'process', 'the', 'result', 'must', 'written', 'file', 'process', 'the', 'comput', 'time', 'parallel', 'program', 'must', 'faster', 'sequenti', 'solut', 'be', 'readi', 'demonstr', 'solut', 'run', 'vlab', 'two', 'comput', 'differ', 'pool', 'use', 'mpi', 'no', 'code', 'share', 'student', 'allow', 'each', 'part', 'code', 'incorpor', 'project', 'must', 'referenc', 'accord', 'academ', 'rule', 'be', 'abl', 'explain', 'line', 'project', 'code', 'includ', 'reus', 'sourc', 'the', 'project', 'creat', 'properli', 'miss', 'file', 'build', 'run', 'error', 'accept', 'input', 'data', 'output', 'result', 'project', 'the', 'input', 'file', 'contain', 'n', 'first', 'line', 'number', 'point', 'set', 'k', 'minim', 'number', 'point', 'satisfi', 'proxim', 'criteria', 'distanc', 'd', 'tcount', 't', 'next', 'n', 'line', 'contain', 'paramet', 'everi', 'point', 'set', 'one', 'blank', 'number', 'file', 'the', 'output', 'file', 'contain', 'inform', 'result', 'found', 'point', 'satisfi', 'proxim', 'criteria', 'for', '3', 'point', 'satisfi', 'proxim', 'criteria', 'found', 'contain', 'line', 'paramet', 'id', '3', 'point', 'point', 'pointid1', 'pointid2', 'pointid3', 'satisfi', 'proxim', 'criteria', 't1', 'point', 'pointid4', 'pointid5', 'pointid6', 'satisfi', 'proxim', 'criteria', 't2', 'point', 'pointid7', 'pointid8', 'pointid9', 'satisfi', 'proxim', 'criteria', 't3', 'in', 'case', 'point', 'found', 'program', 'output', 'number', 'point']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print word statistics on the scraped data before and after text processing."
      ],
      "metadata": {
        "id": "HXn1GWpIsvF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print statistics\n",
        "def print_statistics(title, word_list):\n",
        "    counter = Counter(word_list)\n",
        "    total_words = len(word_list)\n",
        "    unique_words = len(counter)\n",
        "    most_common = counter.most_common(5)\n",
        "    single_occurrences = len([word for word, count in counter.items() if count == 1])\n",
        "\n",
        "    print(f\"\\n{title}\")\n",
        "    print(f\"Total Words: {total_words}\")\n",
        "    print(f\"Unique Words: {unique_words}\")\n",
        "    print(f\"Most Frequent Words: {most_common}\")\n",
        "    print(f\"Words that appear only once: {single_occurrences}\")\n",
        "\n",
        "print_statistics(\"Before processing statistics\", text_content)\n",
        "print_statistics(\"After processing statistics\", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPONP2ytrvas",
        "outputId": "4896eeaa-a996-4cad-9f24-9a6811e4413a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before processing statistics\n",
            "Total Words: 2661\n",
            "Unique Words: 72\n",
            "Most Frequent Words: [(' ', 461), ('t', 246), ('e', 217), ('i', 185), ('o', 157)]\n",
            "Words that appear only once: 15\n",
            "\n",
            "After processing statistics\n",
            "Total Words: 275\n",
            "Unique Words: 139\n",
            "Most Frequent Words: [('point', 20), ('proxim', 11), ('criteria', 11), ('satisfi', 9), ('the', 6)]\n",
            "Words that appear only once: 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WhatsApp Analysis**"
      ],
      "metadata": {
        "id": "98oppp-s30yF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import a .txt file of at least 50 WhatsApp messages in Hebrew."
      ],
      "metadata": {
        "id": "lxjsz16R34e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define the file path to your exported WhatsApp chat\n",
        "file_path = '/content/drive/MyDrive/_chat.txt'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    chat_data = file.read()\n",
        "\n",
        "# Define a regex pattern to match individual messages\n",
        "pattern = re.compile(r'\\[(\\d{2}\\.\\d{2}\\.\\d{4}, \\d{2}:\\d{2}:\\d{2})\\] (.*?): (.*)')\n",
        "\n",
        "# Find all messages using the regex pattern\n",
        "messages = pattern.findall(chat_data)\n",
        "\n",
        "# Convert tuples to lists\n",
        "messages = [list(msg) for msg in messages]\n",
        "\n",
        "# Extract the last 60 messages\n",
        "last_60_messages = messages[-60:]\n",
        "\n",
        "# Flatten the list of lists to a single list\n",
        "flattened_messages = [item for sublist in last_60_messages for item in sublist]\n",
        "\n",
        "# Display the flattened list of messages\n",
        "print(flattened_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRyrK4Xauj5p",
        "outputId": "3c4f3d7f-9d50-4bfc-c3ae-0dc9c4c92a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['21.12.2023, 11:22:46', '~\\u202fNoam ğŸ¦¥', '\\u202b\\u200f~\\u202fNoam ğŸ¦¥ ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '21.12.2023, 11:23:51', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'ğŸ¤×‘×¨×•×›×•×ª ×•×‘×¨×•×›×™× ×”×‘××™× ×œ×›×œ ×”××¦×˜×¨×¤×™× ×‘×©×¢×•×ª ×”××—×¨×•× ×•×ª. ×œ×˜×•×‘×ª ××™ ×©×”×¦×˜×¨×£ ×œ××—×¨×•× ×”, ×× ×™ ××©×ª×¤×ª ××©×¨×•×ª ×©× ×©×œ×—×• ××•×§×“×Ÿ ×™×•×ª×¨ ×•×œ××—×¨ ××›×Ÿ ×™×©×•×ª×¤×• ××©×¨×•×ª ×—×“×©×•×ª.', '21.12.2023, 11:24:48', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://www.linkedin.com/posts/paz-levin-3765751b1_join-my-team-in-the-heart-of-tel-aviv-activity-7140969976399642624-gx3_?utm_source=share&utm_medium=member_android', '21.12.2023, 11:24:48', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×©×œ×•× ×œ×›×•×œ×, ', '21.12.2023, 11:24:49', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×”×‘×•×’×¨ ×× ×“×¨×™ ×¡×™××¤×™×Ÿ ××—×¤×© ×¡×˜×•×“× ×˜×™× ××• ×‘×•×’×¨×™× ××”× ×“×¡×” ×¨×¤×•××™×ª ×œ×‘×™×¦×•×¢ ××©×™××” ×–×× ×™×ª ×œ×‘×“×™×§×•×ª ×•×œ×™×“×¦×™×” ×©×œ ×”××›×©×•×¨ ×”×¨×¤×•××™ - ××™ ×©××¢×•× ×™×™×Ÿ ××• ××¢×•× ×™×™× ×ª - ×¤× ×• ××œ ×× ×“×¨×™:', '21.12.2023, 11:24:49', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '××©×¨×•×ª ××”×‘×•×’×¨ ××‘×™ ×œ×•×™ :', '21.12.2023, 11:24:50', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://www.metacareers.com/jobs/?offices', '21.12.2023, 11:24:51', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '××©×¨×•×ª ××”×‘×•×’×¨ ×ª×•××¨ ×“× ×™××œ- ×”×§×™×©×•×¨ ×”×•× ×œ×œ×™× ×§×“××™×Ÿ ×©×œ ×ª×•××¨', '21.12.2023, 11:25:59', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '\\u200f×”×ª××•× ×” ×”×•×©××˜×”', '21.12.2023, 11:26:00', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×¦×¨×• ×§×©×¨ ×¢× ×¢×•××¨ ×›×¡×¤×™, ×‘×•×’×¨ ×•××•×¡××š ××¤×§×”: https://www.linkedin.com/in/omer-kaspi-phd?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app', '21.12.2023, 11:26:31', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××©×¨×•×ª ×—×“×©×•×ª*:', '21.12.2023, 11:30:07', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://syqe.com/careers#positions_con', '21.12.2023, 11:33:51', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://securithings.com/careers/#career_positions', '21.12.2023, 12:08:47', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×‘×—×‘×¨×ª Utopia Tech Corp  - ××—×¤×©×™× ×¤×¨×•×“××§×˜ ×ª×•×ª×—, ×× ×™×© ×¨×§×¢ ×‘×¤×™× × ×¡×™× / ×ª×©×œ×•××™× / ××¢×¨×›×•×ª ×‘× ×§××™×•×ª ×–×” ×™×ª×¨×•×Ÿ.', '21.12.2023, 16:37:02', '\\u202a+972\\xa052â€‘536â€‘0939\\u202c', '\\u202b\\u200f\\u202a+972\\xa052â€‘536â€‘0939\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '21.12.2023, 16:40:08', '~\\u202fAviv Azar', '\\u202b\\u200f~\\u202fAviv Azar ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '21.12.2023, 16:41:41', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://www.linkedin.com/jobs/view/3778430285/', '24.12.2023, 10:32:29', '~\\u202fGal Ezra', '\\u202b\\u200f~\\u202fGal Ezra ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '24.12.2023, 10:33:02', '~\\u202fSheril Doron', '\\u202b\\u200f~\\u202fSheril Doron ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '24.12.2023, 10:33:27', '\\u202a+972\\xa052â€‘593â€‘3943\\u202c', '\\u202b\\u200f\\u202a+972\\xa052â€‘593â€‘3943\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '24.12.2023, 10:47:34', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '\\u202b\\u200f×ª×™××•×¨ ×”×§×‘×•×¦×” ×©×•× ×” ×¢×œ ×™×“×™ ×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ\\u202c', '24.12.2023, 10:42:17', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '××©×¨×” ×‘×—×‘×¨×ª Ceragon Networks:', '24.12.2023, 10:50:04', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://careers.checkpoint.com/index.php?m=cpcareers&a=search', '24.12.2023, 10:52:01', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://careers.westpharma.com/content/Israel/?locale=en_US', '24.12.2023, 11:04:21', '\\u202a+972\\xa054â€‘814â€‘1912\\u202c', '\\u202b\\u200f\\u202a+972\\xa054â€‘814â€‘1912\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '24.12.2023, 11:08:28', '~\\u202fKobi Sultan', '\\u202b\\u200f~\\u202fKobi Sultan ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '24.12.2023, 11:20:10', '\\u202a+972\\xa050â€‘222â€‘1668\\u202c', '\\u202b\\u200f\\u202a+972\\xa050â€‘222â€‘1668\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '24.12.2023, 11:53:11', '××•×¤×™×¨ ×‘×ª ×”×–×•×’ ×©×œ × ×™×ª××™', '\\u202b\\u200f×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ ×¦×™×¨×£/×” ××ª ××•×¤×™×¨ ×‘×ª ×”×–×•×’ ×©×œ × ×™×ª××™\\u202c', '24.12.2023, 23:50:39', '~\\u202fEtai Leers', '\\u202b\\u200f~\\u202fEtai Leers ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '25.12.2023, 15:39:38', '~\\u202fOmer Ravid', '\\u202b\\u200f~\\u202fOmer Ravid ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '25.12.2023, 15:41:53', '\\u202a+972\\xa050â€‘961â€‘3506\\u202c', '\\u202b\\u200f\\u202a+972\\xa050â€‘961â€‘3506\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '25.12.2023, 15:42:32', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'ğŸ¤×‘×¨×•×›×•×ª ×•×‘×¨×•×›×™× ×”×‘××™× ×œ×›×œ ×”××¦×˜×¨×¤×™× ×‘×©×¢×•×ª ×”××—×¨×•× ×•×ª. ×œ×˜×•×‘×ª ××™ ×©×”×¦×˜×¨×£ ×œ××—×¨×•× ×”, ×× ×™ ××©×ª×¤×ª ××©×¨×•×ª ×©× ×©×œ×—×• ××•×§×“× ×™×•×ª×¨ ×•×œ××—×¨ ××›×Ÿ ×™×©×•×ª×¤×• ××©×¨×•×ª ×—×“×©×•×ª.', '25.12.2023, 15:43:01', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://syqe.com/careers#positions_con', '25.12.2023, 15:43:01', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://securithings.com/careers/#career_positions', '25.12.2023, 15:43:01', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×‘×—×‘×¨×ª Utopia Tech Corp  - ××—×¤×©×™× ×¤×¨×•×“××§×˜ ×ª×•×ª×—, ×× ×™×© ×¨×§×¢ ×‘×¤×™× × ×¡×™× / ×ª×©×œ×•××™× / ××¢×¨×›×•×ª ×‘× ×§××™×•×ª ×–×” ×™×ª×¨×•×Ÿ.', '25.12.2023, 15:43:02', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://www.linkedin.com/jobs/view/3778430285/', '25.12.2023, 15:43:03', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '××©×¨×” ×‘×—×‘×¨×ª Ceragon Networks:', '25.12.2023, 15:43:03', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://careers.checkpoint.com/index.php?m=cpcareers&a=search', '25.12.2023, 15:43:04', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://careers.westpharma.com/content/Israel/?locale=en_US', '25.12.2023, 15:44:39', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×—×•×–×¨×™× ×œ×§××¤×•×¡ ××¤×§×” ×•×”×¤×¢×, ×› *××”× ×“×¡×™ ×¤×™×ª×•×— ××¢×‘×“×•×ª ×œ×—×©××œ ××œ×§×˜×¨×•× ×™×§×” ×•×‘×§×¨×”*', '25.12.2023, 15:44:52', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××©×¨×•×ª ×—×“×©×•×ª*: \\u200f<×”×”×•×“×¢×” × ×¢×¨×›×”>', '25.12.2023, 15:48:26', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://wsc-sports.com/careers/co/israel/38.14B/data-scientist/all/?coref=1.10.rCE_90E&t=1703439384232', '25.12.2023, 16:05:18', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××©×¨×•×ª ×‘×—×‘×¨×ª ××™×§×¨×•×¡×•×¤×˜ ×“×¨×š ×‘×•×’×¨ ××¤×§×” ×™×¨×™×‘ ××™××•×Ÿ*:', '25.12.2023, 18:32:17', '~\\u202fNiso Mazuz', '\\u202b\\u200f~\\u202fNiso Mazuz ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '26.12.2023, 14:38:50', '~\\u202fMá´€Ê€á´›ÉªÉ´ LÉªá´ á´‡Ê€á´€É´á´›', '\\u202b\\u200f~\\u202fMá´€Ê€á´›ÉªÉ´ LÉªá´ á´‡Ê€á´€É´á´› ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '26.12.2023, 14:40:30', '\\u202a+972\\xa054â€‘793â€‘5746\\u202c', '\\u202b\\u200f\\u202a+972\\xa054â€‘793â€‘5746\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '26.12.2023, 14:42:55', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', 'https://www.linkedin.com/posts/activity-7145094333883490304-PC6j?utm_source=share&utm_medium=member_desktop', '26.12.2023, 16:59:19', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××©×¨×•×ª ×‘×—×‘×¨×ª ××•×‘×™×œ××™×™ ××”×‘×•×’×¨×ª ×‘×•×¡×™×” ×¦×œ××Ÿ (×‘×•×¡×™×” ×’× ×”××¨××™×™× ×ª):*', '26.12.2023, 17:03:52', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××©×¨×” ×‘××›×‘×™ ×©×™×¨×•×ª×™ ×‘×¨×™××•×ª*', '27.12.2023, 12:28:52', '\\u202a+972\\xa050â€‘772â€‘3299\\u202c', '\\u202b\\u200f\\u202a+972\\xa050â€‘772â€‘3299\\u202c ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '27.12.2023, 12:33:35', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '××©×¨×” ××”×‘×•×’×¨ ×’×™× ××˜×•×Ÿ:', '27.12.2023, 13:39:44', '~\\u202fOri Matarasso', '\\u202b\\u200f~\\u202fOri Matarasso ×”×¦×˜×¨×£/×” ×œ×§×‘×•×¦×” ×‘×××¦×¢×•×ª ×§×™×©×•×¨ ×”×”×–×× ×”\\u202c', '27.12.2023, 14:48:15', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*Mechanical Engineer*', '27.12.2023, 16:29:16', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××©×¨×•×ª ×‘×—×‘×¨×ª Tabnine :*', '28.12.2023, 11:47:01', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '×”×‘×•×’×¨ ××™×¨× ×œ×•×™× ×’×¨ ××’×™×™×¡ ×œ××’×•×•×Ÿ ××©×¨×•×ª ×‘×—×‘×¨×ª NICE.', '28.12.2023, 16:49:26', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '\\u200f×”×ª××•× ×” ×”×•×©××˜×”', '28.12.2023, 16:52:14', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××§×¤×™×¦×” ×©×•×‘ ××ª ×”××©×¨×” ×œ×‘×§×©×ª ××™×ª×™*', '28.12.2023, 16:57:04', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*××§×¤×™×¦×” ×©×•×‘ ×œ×‘×§×©×ª ×”×‘×•×’×¨:*', '28.12.2023, 17:00:33', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*Customer Support Specialist*  https://wsc-sports.com/careers/co/israel/D2.042/customer-support-specialist/all?coref=1.13.rCE_90E&t=1703775151072', '28.12.2023, 17:08:47', '×“×’× ×™×ª ×¦×™×˜×¨×™×Ÿ ×‘×¨-××•×Ÿ', '*Deputy COO ×‘×—×‘×¨×ª CartaSense* ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize, lemmatize, and stem the WhatsApp data."
      ],
      "metadata": {
        "id": "3GFUZ1hY831O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_messages = ','.join(flattened_messages)\n",
        "# Tokenization\n",
        "tokens = word_tokenize(copy_messages)\n",
        "tokens = [word.lower() for word in tokens if word.isalnum() and word not in stop_words and word not in punctuation]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Lemmatization\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmas:\", lemmas)\n",
        "\n",
        "# Stemming\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stems:\", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7OnwI9k8QTf",
        "outputId": "9a700dd7-58a0-40fa-b4d0-90bc4d524b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['noam', 'noam', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×•×‘×¨×•×›×™×', '×”×‘××™×', '×œ×›×œ', '×”××¦×˜×¨×¤×™×', '×‘×©×¢×•×ª', '×”××—×¨×•× ×•×ª', '×œ×˜×•×‘×ª', '××™', '×©×”×¦×˜×¨×£', '×œ××—×¨×•× ×”', '×× ×™', '××©×ª×¤×ª', '××©×¨×•×ª', '×©× ×©×œ×—×•', '××•×§×“×Ÿ', '×™×•×ª×¨', '×•×œ××—×¨', '××›×Ÿ', '×™×©×•×ª×¤×•', '××©×¨×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×©×œ×•×', '×œ×›×•×œ×', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×‘×•×’×¨', '×× ×“×¨×™', '×¡×™××¤×™×Ÿ', '××—×¤×©', '×¡×˜×•×“× ×˜×™×', '××•', '×‘×•×’×¨×™×', '××”× ×“×¡×”', '×¨×¤×•××™×ª', '×œ×‘×™×¦×•×¢', '××©×™××”', '×–×× ×™×ª', '×œ×‘×“×™×§×•×ª', '×•×œ×™×“×¦×™×”', '×©×œ', '×”××›×©×•×¨', '×”×¨×¤×•××™', '××™', '×©××¢×•× ×™×™×Ÿ', '××•', '××¢×•× ×™×™× ×ª', '×¤× ×•', '××œ', '×× ×“×¨×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '××”×‘×•×’×¨', '××‘×™', '×œ×•×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '××”×‘×•×’×¨', '×ª×•××¨', '×”×§×™×©×•×¨', '×”×•×', '×œ×œ×™× ×§×“××™×Ÿ', '×©×œ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×¦×¨×•', '×§×©×¨', '×¢×', '×¢×•××¨', '×›×¡×¤×™', '×‘×•×’×¨', '×•××•×¡××š', '××¤×§×”', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×—×“×©×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×‘×—×‘×¨×ª', 'utopia', 'tech', 'corp', '××—×¤×©×™×', '×¤×¨×•×“××§×˜', '×ª×•×ª×—', '××', '×™×©', '×¨×§×¢', '×‘×¤×™× × ×¡×™×', '×ª×©×œ×•××™×', '××¢×¨×›×•×ª', '×‘× ×§××™×•×ª', '×–×”', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'aviv', 'azar', 'aviv', 'azar', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', 'gal', 'ezra', 'gal', 'ezra', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'sheril', 'doron', 'sheril', 'doron', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×§×‘×•×¦×”', '×©×•× ×”', '×¢×œ', '×™×“×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘×—×‘×¨×ª', 'ceragon', 'networks', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'kobi', 'sultan', 'kobi', 'sultan', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '××•×¤×™×¨', '×‘×ª', '×”×–×•×’', '×©×œ', '× ×™×ª××™', '×¦×™×˜×¨×™×Ÿ', '××ª', '××•×¤×™×¨', '×‘×ª', '×”×–×•×’', '×©×œ', 'etai', 'leers', 'etai', 'leers', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'omer', 'ravid', 'omer', 'ravid', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×•×‘×¨×•×›×™×', '×”×‘××™×', '×œ×›×œ', '×”××¦×˜×¨×¤×™×', '×‘×©×¢×•×ª', '×”××—×¨×•× ×•×ª', '×œ×˜×•×‘×ª', '××™', '×©×”×¦×˜×¨×£', '×œ××—×¨×•× ×”', '×× ×™', '××©×ª×¤×ª', '××©×¨×•×ª', '×©× ×©×œ×—×•', '××•×§×“×', '×™×•×ª×¨', '×•×œ××—×¨', '××›×Ÿ', '×™×©×•×ª×¤×•', '××©×¨×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×‘×—×‘×¨×ª', 'utopia', 'tech', 'corp', '××—×¤×©×™×', '×¤×¨×•×“××§×˜', '×ª×•×ª×—', '××', '×™×©', '×¨×§×¢', '×‘×¤×™× × ×¡×™×', '×ª×©×œ×•××™×', '××¢×¨×›×•×ª', '×‘× ×§××™×•×ª', '×–×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘×—×‘×¨×ª', 'ceragon', 'networks', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×—×•×–×¨×™×', '×œ×§××¤×•×¡', '××¤×§×”', '×•×”×¤×¢×', '×›', '××”× ×“×¡×™', '×¤×™×ª×•×—', '××¢×‘×“×•×ª', '×œ×—×©××œ', '××œ×§×˜×¨×•× ×™×§×”', '×•×‘×§×¨×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×—×“×©×•×ª', '×”×”×•×“×¢×”', '× ×¢×¨×›×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '××™×§×¨×•×¡×•×¤×˜', '×“×¨×š', '×‘×•×’×¨', '××¤×§×”', '×™×¨×™×‘', '××™××•×Ÿ', 'niso', 'mazuz', 'niso', 'mazuz', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'má´€Ê€á´›ÉªÉ´', 'lÉªá´ á´‡Ê€á´€É´á´›', 'má´€Ê€á´›ÉªÉ´', 'lÉªá´ á´‡Ê€á´€É´á´›', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '××•×‘×™×œ××™×™', '××”×‘×•×’×¨×ª', '×‘×•×¡×™×”', '×¦×œ××Ÿ', '×‘×•×¡×™×”', '×’×', '×”××¨××™×™× ×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘××›×‘×™', '×©×™×¨×•×ª×™', '×‘×¨×™××•×ª', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '××”×‘×•×’×¨', '×’×™×', '××˜×•×Ÿ', 'ori', 'matarasso', 'ori', 'matarasso', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'mechanical', 'engineer', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', 'tabnine', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×‘×•×’×¨', '××™×¨×', '×œ×•×™× ×’×¨', '××’×™×™×¡', '×œ××’×•×•×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××§×¤×™×¦×”', '×©×•×‘', '××ª', '×”××©×¨×”', '×œ×‘×§×©×ª', '××™×ª×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××§×¤×™×¦×”', '×©×•×‘', '×œ×‘×§×©×ª', '×”×‘×•×’×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'customer', 'support', 'specialist', 'https', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'deputy', 'coo', '×‘×—×‘×¨×ª', 'cartasense']\n",
            "Lemmas: ['noam', 'noam', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×•×‘×¨×•×›×™×', '×”×‘××™×', '×œ×›×œ', '×”××¦×˜×¨×¤×™×', '×‘×©×¢×•×ª', '×”××—×¨×•× ×•×ª', '×œ×˜×•×‘×ª', '××™', '×©×”×¦×˜×¨×£', '×œ××—×¨×•× ×”', '×× ×™', '××©×ª×¤×ª', '××©×¨×•×ª', '×©× ×©×œ×—×•', '××•×§×“×Ÿ', '×™×•×ª×¨', '×•×œ××—×¨', '××›×Ÿ', '×™×©×•×ª×¤×•', '××©×¨×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×©×œ×•×', '×œ×›×•×œ×', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×‘×•×’×¨', '×× ×“×¨×™', '×¡×™××¤×™×Ÿ', '××—×¤×©', '×¡×˜×•×“× ×˜×™×', '××•', '×‘×•×’×¨×™×', '××”× ×“×¡×”', '×¨×¤×•××™×ª', '×œ×‘×™×¦×•×¢', '××©×™××”', '×–×× ×™×ª', '×œ×‘×“×™×§×•×ª', '×•×œ×™×“×¦×™×”', '×©×œ', '×”××›×©×•×¨', '×”×¨×¤×•××™', '××™', '×©××¢×•× ×™×™×Ÿ', '××•', '××¢×•× ×™×™× ×ª', '×¤× ×•', '××œ', '×× ×“×¨×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '××”×‘×•×’×¨', '××‘×™', '×œ×•×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '××”×‘×•×’×¨', '×ª×•××¨', '×”×§×™×©×•×¨', '×”×•×', '×œ×œ×™× ×§×“××™×Ÿ', '×©×œ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×¦×¨×•', '×§×©×¨', '×¢×', '×¢×•××¨', '×›×¡×¤×™', '×‘×•×’×¨', '×•××•×¡××š', '××¤×§×”', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×—×“×©×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×‘×—×‘×¨×ª', 'utopia', 'tech', 'corp', '××—×¤×©×™×', '×¤×¨×•×“××§×˜', '×ª×•×ª×—', '××', '×™×©', '×¨×§×¢', '×‘×¤×™× × ×¡×™×', '×ª×©×œ×•××™×', '××¢×¨×›×•×ª', '×‘× ×§××™×•×ª', '×–×”', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'aviv', 'azar', 'aviv', 'azar', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', 'gal', 'ezra', 'gal', 'ezra', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'sheril', 'doron', 'sheril', 'doron', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×§×‘×•×¦×”', '×©×•× ×”', '×¢×œ', '×™×“×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘×—×‘×¨×ª', 'ceragon', 'network', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'kobi', 'sultan', 'kobi', 'sultan', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '××•×¤×™×¨', '×‘×ª', '×”×–×•×’', '×©×œ', '× ×™×ª××™', '×¦×™×˜×¨×™×Ÿ', '××ª', '××•×¤×™×¨', '×‘×ª', '×”×–×•×’', '×©×œ', 'etai', 'leer', 'etai', 'leer', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'omer', 'ravid', 'omer', 'ravid', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×•×‘×¨×•×›×™×', '×”×‘××™×', '×œ×›×œ', '×”××¦×˜×¨×¤×™×', '×‘×©×¢×•×ª', '×”××—×¨×•× ×•×ª', '×œ×˜×•×‘×ª', '××™', '×©×”×¦×˜×¨×£', '×œ××—×¨×•× ×”', '×× ×™', '××©×ª×¤×ª', '××©×¨×•×ª', '×©× ×©×œ×—×•', '××•×§×“×', '×™×•×ª×¨', '×•×œ××—×¨', '××›×Ÿ', '×™×©×•×ª×¤×•', '××©×¨×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×‘×—×‘×¨×ª', 'utopia', 'tech', 'corp', '××—×¤×©×™×', '×¤×¨×•×“××§×˜', '×ª×•×ª×—', '××', '×™×©', '×¨×§×¢', '×‘×¤×™× × ×¡×™×', '×ª×©×œ×•××™×', '××¢×¨×›×•×ª', '×‘× ×§××™×•×ª', '×–×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘×—×‘×¨×ª', 'ceragon', 'network', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×—×•×–×¨×™×', '×œ×§××¤×•×¡', '××¤×§×”', '×•×”×¤×¢×', '×›', '××”× ×“×¡×™', '×¤×™×ª×•×—', '××¢×‘×“×•×ª', '×œ×—×©××œ', '××œ×§×˜×¨×•× ×™×§×”', '×•×‘×§×¨×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×—×“×©×•×ª', '×”×”×•×“×¢×”', '× ×¢×¨×›×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '××™×§×¨×•×¡×•×¤×˜', '×“×¨×š', '×‘×•×’×¨', '××¤×§×”', '×™×¨×™×‘', '××™××•×Ÿ', 'niso', 'mazuz', 'niso', 'mazuz', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'má´€Ê€á´›ÉªÉ´', 'lÉªá´ á´‡Ê€á´€É´á´›', 'má´€Ê€á´›ÉªÉ´', 'lÉªá´ á´‡Ê€á´€É´á´›', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '××•×‘×™×œ××™×™', '××”×‘×•×’×¨×ª', '×‘×•×¡×™×”', '×¦×œ××Ÿ', '×‘×•×¡×™×”', '×’×', '×”××¨××™×™× ×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘××›×‘×™', '×©×™×¨×•×ª×™', '×‘×¨×™××•×ª', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '××”×‘×•×’×¨', '×’×™×', '××˜×•×Ÿ', 'ori', 'matarasso', 'ori', 'matarasso', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'mechanical', 'engineer', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', 'tabnine', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×‘×•×’×¨', '××™×¨×', '×œ×•×™× ×’×¨', '××’×™×™×¡', '×œ××’×•×•×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××§×¤×™×¦×”', '×©×•×‘', '××ª', '×”××©×¨×”', '×œ×‘×§×©×ª', '××™×ª×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××§×¤×™×¦×”', '×©×•×‘', '×œ×‘×§×©×ª', '×”×‘×•×’×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'customer', 'support', 'specialist', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'deputy', 'coo', '×‘×—×‘×¨×ª', 'cartasense']\n",
            "Stems: ['noam', 'noam', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×•×‘×¨×•×›×™×', '×”×‘××™×', '×œ×›×œ', '×”××¦×˜×¨×¤×™×', '×‘×©×¢×•×ª', '×”××—×¨×•× ×•×ª', '×œ×˜×•×‘×ª', '××™', '×©×”×¦×˜×¨×£', '×œ××—×¨×•× ×”', '×× ×™', '××©×ª×¤×ª', '××©×¨×•×ª', '×©× ×©×œ×—×•', '××•×§×“×Ÿ', '×™×•×ª×¨', '×•×œ××—×¨', '××›×Ÿ', '×™×©×•×ª×¤×•', '××©×¨×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×©×œ×•×', '×œ×›×•×œ×', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×‘×•×’×¨', '×× ×“×¨×™', '×¡×™××¤×™×Ÿ', '××—×¤×©', '×¡×˜×•×“× ×˜×™×', '××•', '×‘×•×’×¨×™×', '××”× ×“×¡×”', '×¨×¤×•××™×ª', '×œ×‘×™×¦×•×¢', '××©×™××”', '×–×× ×™×ª', '×œ×‘×“×™×§×•×ª', '×•×œ×™×“×¦×™×”', '×©×œ', '×”××›×©×•×¨', '×”×¨×¤×•××™', '××™', '×©××¢×•× ×™×™×Ÿ', '××•', '××¢×•× ×™×™× ×ª', '×¤× ×•', '××œ', '×× ×“×¨×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '××”×‘×•×’×¨', '××‘×™', '×œ×•×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '××”×‘×•×’×¨', '×ª×•××¨', '×”×§×™×©×•×¨', '×”×•×', '×œ×œ×™× ×§×“××™×Ÿ', '×©×œ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×¦×¨×•', '×§×©×¨', '×¢×', '×¢×•××¨', '×›×¡×¤×™', '×‘×•×’×¨', '×•××•×¡××š', '××¤×§×”', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×—×“×©×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×‘×—×‘×¨×ª', 'utopia', 'tech', 'corp', '××—×¤×©×™×', '×¤×¨×•×“××§×˜', '×ª×•×ª×—', '××', '×™×©', '×¨×§×¢', '×‘×¤×™× × ×¡×™×', '×ª×©×œ×•××™×', '××¢×¨×›×•×ª', '×‘× ×§××™×•×ª', '×–×”', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'aviv', 'azar', 'aviv', 'azar', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', 'gal', 'ezra', 'gal', 'ezra', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'sheril', 'doron', 'sheril', 'doron', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×§×‘×•×¦×”', '×©×•× ×”', '×¢×œ', '×™×“×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘×—×‘×¨×ª', 'ceragon', 'network', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'kobi', 'sultan', 'kobi', 'sultan', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '××•×¤×™×¨', '×‘×ª', '×”×–×•×’', '×©×œ', '× ×™×ª××™', '×¦×™×˜×¨×™×Ÿ', '××ª', '××•×¤×™×¨', '×‘×ª', '×”×–×•×’', '×©×œ', 'etai', 'leer', 'etai', 'leer', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'omer', 'ravid', 'omer', 'ravid', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×•×‘×¨×•×›×™×', '×”×‘××™×', '×œ×›×œ', '×”××¦×˜×¨×¤×™×', '×‘×©×¢×•×ª', '×”××—×¨×•× ×•×ª', '×œ×˜×•×‘×ª', '××™', '×©×”×¦×˜×¨×£', '×œ××—×¨×•× ×”', '×× ×™', '××©×ª×¤×ª', '××©×¨×•×ª', '×©× ×©×œ×—×•', '××•×§×“×', '×™×•×ª×¨', '×•×œ××—×¨', '××›×Ÿ', '×™×©×•×ª×¤×•', '××©×¨×•×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×‘×—×‘×¨×ª', 'utopia', 'tech', 'corp', '××—×¤×©×™×', '×¤×¨×•×“××§×˜', '×ª×•×ª×—', '××', '×™×©', '×¨×§×¢', '×‘×¤×™× × ×¡×™×', '×ª×©×œ×•××™×', '××¢×¨×›×•×ª', '×‘× ×§××™×•×ª', '×–×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘×—×‘×¨×ª', 'ceragon', 'network', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×—×•×–×¨×™×', '×œ×§××¤×•×¡', '××¤×§×”', '×•×”×¤×¢×', '×›', '××”× ×“×¡×™', '×¤×™×ª×•×—', '××¢×‘×“×•×ª', '×œ×—×©××œ', '××œ×§×˜×¨×•× ×™×§×”', '×•×‘×§×¨×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×—×“×©×•×ª', '×”×”×•×“×¢×”', '× ×¢×¨×›×”', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '××™×§×¨×•×¡×•×¤×˜', '×“×¨×š', '×‘×•×’×¨', '××¤×§×”', '×™×¨×™×‘', '××™××•×Ÿ', 'niso', 'mazuz', 'niso', 'mazuz', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', 'má´€Ê€á´›ÉªÉ´', 'lÉªá´ á´‡Ê€á´€É´á´›', 'má´€Ê€á´›ÉªÉ´', 'lÉªá´ á´‡Ê€á´€É´á´›', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '××•×‘×™×œ××™×™', '××”×‘×•×’×¨×ª', '×‘×•×¡×™×”', '×¦×œ××Ÿ', '×‘×•×¡×™×”', '×’×', '×”××¨××™×™× ×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '×‘××›×‘×™', '×©×™×¨×•×ª×™', '×‘×¨×™××•×ª', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×”', '××”×‘×•×’×¨', '×’×™×', '××˜×•×Ÿ', 'ori', 'matarasso', 'ori', 'matarasso', '×œ×§×‘×•×¦×”', '×‘×××¦×¢×•×ª', '×§×™×©×•×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'mechan', 'engin', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', 'tabnin', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×”×‘×•×’×¨', '××™×¨×', '×œ×•×™× ×’×¨', '××’×™×™×¡', '×œ××’×•×•×Ÿ', '××©×¨×•×ª', '×‘×—×‘×¨×ª', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××§×¤×™×¦×”', '×©×•×‘', '××ª', '×”××©×¨×”', '×œ×‘×§×©×ª', '××™×ª×™', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', '××§×¤×™×¦×”', '×©×•×‘', '×œ×‘×§×©×ª', '×”×‘×•×’×¨', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'custom', 'support', 'specialist', 'http', '×“×’× ×™×ª', '×¦×™×˜×¨×™×Ÿ', 'deputi', 'coo', '×‘×—×‘×¨×ª', 'cartasens']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print comparisons of word statistics before and after processing."
      ],
      "metadata": {
        "id": "wDUCBLGVffNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_statistics(\"Before processing statistics\", ','.join(flattened_messages))\n",
        "print_statistics(\"After processing statistics\", stems)"
      ],
      "metadata": {
        "id": "8xAtRnpT9IZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0c7064-e207-44f5-a16d-1c1174b0abd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before processing statistics\n",
            "Total Words: 5953\n",
            "Unique Words: 113\n",
            "Most Frequent Words: [(' ', 466), ('2', 318), (',', 246), ('×™', 241), ('×•', 219)]\n",
            "Words that appear only once: 6\n",
            "\n",
            "After processing statistics\n",
            "Total Words: 400\n",
            "Unique Words: 156\n",
            "Most Frequent Words: [('×¦×™×˜×¨×™×Ÿ', 44), ('×“×’× ×™×ª', 43), ('×œ×§×‘×•×¦×”', 17), ('×‘×××¦×¢×•×ª', 17), ('×§×™×©×•×¨', 17)]\n",
            "Words that appear only once: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tbdo6H7w48QS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}